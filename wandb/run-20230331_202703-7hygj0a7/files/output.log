  0%|                                                                                                  | 0/4062 [00:00<?, ?it/s]The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: labels_attention_mask. If labels_attention_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.



















































































































































































































































































































































































































































































































 12%|██████████▌                                                                           | 500/4062 [24:05<2:43:47,  2.76s/it]



















































































































































































































































































































































































































































































































 25%|█████████████████████▏                                                                | 999/4062 [46:49<2:18:48,  2.72s/it]





















































































































































































































































































































































































































































































































 37%|██████████████████████████████▋                                                    | 1500/4062 [1:09:38<2:01:19,  2.84s/it]




















































































































































































































































































































































































































































































































 49%|████████████████████████████████████████▊                                          | 2000/4062 [1:32:22<1:32:35,  2.69s/it]




















































































































































































































































































































































































































































































































 62%|███████████████████████████████████████████████████                                | 2500/4062 [1:55:06<1:14:00,  2.84s/it]




















































































































































































































































































































































































































































































































 74%|█████████████████████████████████████████████████████████████▎                     | 3000/4062 [2:18:09<1:18:07,  4.41s/it]




















































































































































































































































































































































































































































































































 86%|█████████████████████████████████████████████████████████████████████████▏           | 3500/4062 [2:41:22<25:10,  2.69s/it]



















































































































































































































































































































































































































































































































 98%|███████████████████████████████████████████████████████████████████████████████████▋ | 3999/4062 [3:03:58<02:48,  2.68s/it]






























































100%|█████████████████████████████████████████████████████████████████████████████████████| 4062/4062 [3:06:50<00:00,  2.86s/it]
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████| 4062/4062 [3:06:50<00:00,  2.76s/it]
***** Running training *****
  Num examples = 2708
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 4062
  Number of trainable parameters = 124697433
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                  | 0/4062 [00:00<?, ?it/s]The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: labels_attention_mask. If labels_attention_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.
{'train_runtime': 11214.9504, 'train_samples_per_second': 0.724, 'train_steps_per_second': 0.362, 'train_loss': 6.051259539034612, 'epoch': 3.0}
Traceback (most recent call last):
  File "/home/cognitron/codebertcustom/trainarma1.py", line 103, in <module>
    trainer.train()
  File "/home/cognitron/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1543, in train
    return inner_training_loop(
  File "/home/cognitron/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1861, in _inner_training_loop
    self.lr_scheduler.step()
TypeError: ReduceLROnPlateau.step() missing 1 required positional argument: 'metrics'
Traceback (most recent call last):
  File "/home/cognitron/codebertcustom/trainarma1.py", line 103, in <module>
    trainer.train()
  File "/home/cognitron/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1543, in train
    return inner_training_loop(
  File "/home/cognitron/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1861, in _inner_training_loop
    self.lr_scheduler.step()
TypeError: ReduceLROnPlateau.step() missing 1 required positional argument: 'metrics'